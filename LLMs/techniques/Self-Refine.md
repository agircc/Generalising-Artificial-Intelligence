## üîç Problem to Solve

Large Language Models (LLMs) like GPT-4 can generate high-quality outputs, but their **first attempts are not always optimal**‚Äîespecially for tasks that:

* Have vague or multi-dimensional objectives (e.g., writing polite responses),
* Require creativity or constraint satisfaction,
* Or need clear reasoning and structure (e.g., math/code tasks).

Human writers naturally refine their work through **self-feedback and revision**. The paper asks:

> Can LLMs do the same‚Äî**refine their own output through feedback**‚Äîwithout extra training?

---

## üß† What is SELF-REFINE?

**SELF-REFINE** is a *zero-shot*, *training-free*, *iterative* framework that enables an LLM to **improve its own output** by:

1. **Generating** an initial draft for a task.
2. **Providing feedback** on its own output.
3. **Using that feedback** to revise the output.
4. Repeating steps 2 and 3 until a stopping condition is met.

All steps are done using **the same language model**, without any additional training or external supervision.

> Think of it as the model acting as **writer, critic, and editor‚Äîall in one**.

---

## üîß How to Use SELF-REFINE

To implement SELF-REFINE, you need:

* A base LLM (e.g., GPT-3.5, GPT-4)
* Three few-shot prompts:

  1. For initial generation (`pgen`)
  2. For generating feedback (`pfb`)
  3. For refining output based on feedback (`prefine`)

### Iterative Workflow:

1. **Initial Output**: `y‚ÇÄ = M(pgen ‚à• x)`
2. **Feedback**: `fb‚Çú = M(pfb ‚à• x ‚à• y‚Çú)`
3. **Refine**: `y‚Çú‚Çä‚ÇÅ = M(prefine ‚à• x ‚à• y‚Çú ‚à• fb‚Çú)`
4. Repeat until:

   * Max iterations reached, or
   * Model indicates the output is good enough

---

## üß≠ Methodology for Applying SELF-REFINE to Other Scenarios

To adapt SELF-REFINE beyond the paper's tasks (e.g., real-world applications), follow this generalized approach:

### 1. **Identify a Use Case**

Choose a task where:

* Outputs benefit from iterative refinement (e.g., writing, summarizing, translating, designing)
* Quality is subjective or multi-dimensional
* Feedback can be expressed in natural language

### 2. **Design Prompts**

Create three prompts:

* `pgen`: instruct the model to generate a first draft
* `pfb`: instruct the model to review its output and suggest specific, actionable feedback
* `prefine`: instruct the model to rewrite using that feedback

Use few-shot examples if necessary to guide the behavior.

### 3. **Run Iterations**

Loop through feedback and refinement 2‚Äì4 times. Optionally include a stopping condition, like a flag in the feedback indicating "done".

### 4. **Evaluation**

Use human preference or an automated scoring function to select the best version across iterations.

---

## üí° Example Real-World Applications

* **Customer Support Response Drafting**: Improve clarity, tone, and helpfulness.
* **Website Content Generation**: Iteratively refine headlines or body copy for better engagement.
* **Lesson Plan Creation**: Refine draft curriculum to include more learning goals or interactivity.
* **Creative Writing**: Enhance storytelling, pacing, or emotional tone.

---

## ‚úÖ Why It Matters

* No additional training or fine-tuning needed
* Can be used with *any* strong LLM
* Flexible across domains (dialog, code, math, creative text)
* Significantly improves output quality‚Äîby up to **50% in preference-based tasks**

---


## Reference
Madaan, A., Tandon, N., Gupta, P., Hallinan, S., Gao, L., Wiegreffe, S., Alon, U., Dziri, N., Prabhumoye, S., Yang, Y., Gupta, S., Majumder, B. P., Hermann, K., Welleck, S., Yazdanbakhsh, A., & Clark, P. (2023). Self-Refine: Iterative Refinement with Self-Feedback. https://doi.org/10.48550/arxiv.2303.17651
