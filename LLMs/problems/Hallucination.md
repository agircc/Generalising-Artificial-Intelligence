### Hallucination (Factual Inaccuracy)

Description: LLMs often generate text that appears plausible but is factually incorrect or entirely fabricated.

Impact: Dangerous in critical domains like healthcare, law, and education.