Small changes in example plans or prompt phrasing can significantly degrade output quality.

LLMs do not generalize well to structurally similar but lexically different inputs.