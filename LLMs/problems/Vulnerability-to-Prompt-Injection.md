### Vulnerability to Prompt Injection / Jailbreaking


Description: Malicious prompts can manipulate LLMs to bypass safety filters or produce harmful content.

Example: Prompt injection, instruction-hacking, adversarial attacks.